{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9795944,"sourceType":"datasetVersion","datasetId":6003234},{"sourceId":155602,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":132223,"modelId":155021}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:21:44.226874Z","iopub.execute_input":"2024-11-06T08:21:44.227737Z","iopub.status.idle":"2024-11-06T08:21:44.244374Z","shell.execute_reply.started":"2024-11-06T08:21:44.227695Z","shell.execute_reply":"2024-11-06T08:21:44.243457Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/final_ml_models/scikitlearn/default/1/Integrity_Impact_best_catboost_model.pkl\n/kaggle/input/final_ml_models/scikitlearn/default/1/Impact_Score_xgb_regressor.pkl\n/kaggle/input/final_ml_models/scikitlearn/default/1/accessVector_best_catboost_model.pkl\n/kaggle/input/final_ml_models/scikitlearn/default/1/Availability_Impact_best_catboost_model.pkl\n/kaggle/input/final_ml_models/scikitlearn/default/1/Availability_Impact_label_encoder.pkl\n/kaggle/input/final_ml_models/scikitlearn/default/1/Exploitability_Score_xgb_regressor.pkl\n/kaggle/input/final_ml_models/scikitlearn/default/1/Access_Complexity_label_encoder.pkl\n/kaggle/input/final_ml_models/scikitlearn/default/1/Integrity_Impactt_label_encoder.pkl\n/kaggle/input/final_ml_models/scikitlearn/default/1/Access_Complexity_best_catboost_model.pkl\n/kaggle/input/final_ml_models/scikitlearn/default/1/Confidentiality_Impact_best_catboost_model.pkl\n/kaggle/input/final_ml_models/scikitlearn/default/1/Confidentiality_Impact_label_encoder.pkl\n/kaggle/input/final_ml_models/scikitlearn/default/1/Access_Vector_label_encoder.pkl\n/kaggle/input/final_ml_models/scikitlearn/default/1/BASE_SCORE_xgb_regressor.pkl\n/kaggle/input/cve-dataset-with-embedding/merged_cve_data.csv\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"! pip install xgboost\n!pip install lightgbm catboost torch torchvision torchaudio\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:21:45.830252Z","iopub.execute_input":"2024-11-06T08:21:45.830978Z","iopub.status.idle":"2024-11-06T08:22:10.786519Z","shell.execute_reply.started":"2024-11-06T08:21:45.830942Z","shell.execute_reply":"2024-11-06T08:22:10.785299Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: xgboost in /opt/conda/lib/python3.10/site-packages (2.0.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xgboost) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from xgboost) (1.14.1)\nRequirement already satisfied: lightgbm in /opt/conda/lib/python3.10/site-packages (4.2.0)\nRequirement already satisfied: catboost in /opt/conda/lib/python3.10/site-packages (1.2.7)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from lightgbm) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from lightgbm) (1.14.1)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from catboost) (0.20.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from catboost) (3.7.5)\nRequirement already satisfied: pandas>=0.24 in /opt/conda/lib/python3.10/site-packages (from catboost) (2.2.2)\nRequirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from catboost) (5.22.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from catboost) (1.16.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2024.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (3.1.2)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->catboost) (8.3.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/cve-dataset-with-embedding/merged_cve_data.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:22:29.619272Z","iopub.execute_input":"2024-11-06T08:22:29.620222Z","iopub.status.idle":"2024-11-06T08:22:48.110639Z","shell.execute_reply.started":"2024-11-06T08:22:29.620167Z","shell.execute_reply":"2024-11-06T08:22:48.109769Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:22:51.431889Z","iopub.execute_input":"2024-11-06T08:22:51.432340Z","iopub.status.idle":"2024-11-06T08:22:51.530624Z","shell.execute_reply.started":"2024-11-06T08:22:51.432300Z","shell.execute_reply":"2024-11-06T08:22:51.529594Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"               CVE_ID                    ASSIGNER             Published_Date  \\\n0       CVE-2011-0199  product-security@apple.com  2011-06-24 20:55:00+00:00   \n1       CVE-2011-0220  product-security@apple.com  2020-02-05 20:15:00+00:00   \n2       CVE-2011-0428               cve@mitre.org  2019-10-29 19:15:00+00:00   \n3       CVE-2011-0467       security@opentext.com  2018-06-07 21:29:00+00:00   \n4       CVE-2011-0469       security@opentext.com  2017-08-17 16:29:00+00:00   \n...               ...                         ...                        ...   \n107082  CVE-2008-7315               cve@mitre.org  2017-10-10 16:29:00+00:00   \n107083  CVE-2008-7316         security@debian.org  2016-05-02 10:59:00+00:00   \n107084  CVE-2008-7319               cve@mitre.org  2017-11-07 21:29:00+00:00   \n107085  CVE-2008-7320               cve@mitre.org  2018-11-18 19:29:00+00:00   \n107086  CVE-2008-7321               cve@mitre.org  2019-08-22 14:15:00+00:00   \n\n               Last_Modified_Date  Impact_Score Access_Vector  \\\n0       2024-02-09 03:18:00+00:00           3.6       NETWORK   \n1       2020-02-07 19:24:00+00:00           3.6         LOCAL   \n2       2019-11-01 14:55:00+00:00           2.7       NETWORK   \n3       2023-11-07 02:06:00+00:00           5.9       NETWORK   \n4       2023-11-07 02:06:00+00:00           5.9       NETWORK   \n...                           ...           ...           ...   \n107082  2017-11-03 17:15:00+00:00           5.9       NETWORK   \n107083  2016-05-06 00:54:00+00:00           3.6         LOCAL   \n107084  2017-11-29 15:49:00+00:00           5.9       NETWORK   \n107085  2024-08-07 12:15:00+00:00           5.9         LOCAL   \n107086  2019-08-23 19:44:00+00:00           2.7       NETWORK   \n\n       Access_Complexity                                     Configurations  \\\n0                 MEDIUM  ['cpe:2.3:o:apple:mac_os_x_server:*:*:*:*:*:*:...   \n1                    LOW        ['cpe:2.3:a:apple:bonjour:*:*:*:*:*:*:*:*']   \n2                 MEDIUM      ['cpe:2.3:a:ikiwiki:ikiwiki:*:*:*:*:*:*:*:*']   \n3                    LOW  ['cpe:2.3:a:suse:studio_onsite_appliance:*:*:*...   \n4                    LOW        ['cpe:2.3:o:suse:opensuse:-:*:*:*:*:*:*:*']   \n...                  ...                                                ...   \n107082               LOW  ['cpe:2.3:a:cpan:ui\\\\:\\\\:dialog:0.01:*:*:*:*:*...   \n107083               LOW   ['cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*']   \n107084               LOW  ['cpe:2.3:a:net-ping-external_project:net-ping...   \n107085               LOW       ['cpe:2.3:a:gnome:seahorse:*:*:*:*:*:*:*:*']   \n107086            MEDIUM  ['cpe:2.3:a:tubepress:tubepress:*:*:*:*:*:word...   \n\n                                           Reference_Data  Year  ...  \\\n0       ['http://support.apple.com/kb/HT4723', 'http:/...  2011  ...   \n1       ['https://opensource.apple.com/source/mDNSResp...  2020  ...   \n2       ['https://security-tracker.debian.org/tracker/...  2019  ...   \n3       ['https://bugzilla.suse.com/show_bug.cgi?id=67...  2018  ...   \n4       ['https://bugzilla.suse.com/show_bug.cgi?id=67...  2017  ...   \n...                                                   ...   ...  ...   \n107082  ['https://security-tracker.debian.org/tracker/...  2017  ...   \n107083  ['http://git.kernel.org/cgit/linux/kernel/git/...  2016  ...   \n107084  ['https://rt.cpan.org/Public/Bug/Display.html?...  2017  ...   \n107085  ['https://www.bountysource.com/issues/3849352-...  2018  ...   \n107086  ['https://wordpress.org/plugins/tubepress/#dev...  2019  ...   \n\n             759       760       761       762       763       764       765  \\\n0      -0.007224 -0.015303  0.056557 -0.039082  0.008622  0.022517 -0.040034   \n1       0.016330  0.049928  0.008830  0.000793  0.017700  0.028334  0.005227   \n2       0.016057 -0.002453  0.037362 -0.007217 -0.034629 -0.005441 -0.000510   \n3       0.023072 -0.052349  0.029655 -0.052779 -0.018734 -0.022419 -0.001663   \n4       0.008663  0.009789  0.012439 -0.032353  0.007584 -0.005330  0.010918   \n...          ...       ...       ...       ...       ...       ...       ...   \n107082  0.019123 -0.046788  0.001866  0.024574 -0.024319 -0.030054  0.025092   \n107083  0.014300 -0.012750 -0.036155 -0.043011  0.024458  0.012200 -0.009282   \n107084  0.034606 -0.056721 -0.028375 -0.023293 -0.014179 -0.005051 -0.029391   \n107085 -0.022035  0.005189 -0.003003  0.003738  0.012156 -0.027635 -0.001326   \n107086 -0.005189 -0.073709  0.005477  0.011733 -0.017345 -0.009489 -0.022760   \n\n             766       767                                        Description  \n0       0.014473  0.011391  The Certificate Trust Policy component in Appl...  \n1       0.049708 -0.015749  Apple Bonjour before 2011 allows a crash via a...  \n2      -0.015185 -0.025799  Cross Site Scripting (XSS) in ikiwiki before 3...  \n3       0.004668 -0.070693  A vulnerability in the listing of available so...  \n4      -0.033823  0.007783  Code injection in openSUSE when running some s...  \n...          ...       ...                                                ...  \n107082 -0.023828 -0.022433  UI-Dialog 1.09 and earlier allows remote attac...  \n107083  0.023590 -0.018185  mm/filemap.c in the Linux kernel before 2.6.25...  \n107084 -0.020186 -0.033210  The Net::Ping::External extension through 0.15...  \n107085  0.000663 -0.036381  GNOME Seahorse through 3.30 allows physically ...  \n107086  0.000676  0.032018  The tubepress plugin before 1.6.5 for WordPres...  \n\n[107087 rows x 784 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CVE_ID</th>\n      <th>ASSIGNER</th>\n      <th>Published_Date</th>\n      <th>Last_Modified_Date</th>\n      <th>Impact_Score</th>\n      <th>Access_Vector</th>\n      <th>Access_Complexity</th>\n      <th>Configurations</th>\n      <th>Reference_Data</th>\n      <th>Year</th>\n      <th>...</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CVE-2011-0199</td>\n      <td>product-security@apple.com</td>\n      <td>2011-06-24 20:55:00+00:00</td>\n      <td>2024-02-09 03:18:00+00:00</td>\n      <td>3.6</td>\n      <td>NETWORK</td>\n      <td>MEDIUM</td>\n      <td>['cpe:2.3:o:apple:mac_os_x_server:*:*:*:*:*:*:...</td>\n      <td>['http://support.apple.com/kb/HT4723', 'http:/...</td>\n      <td>2011</td>\n      <td>...</td>\n      <td>-0.007224</td>\n      <td>-0.015303</td>\n      <td>0.056557</td>\n      <td>-0.039082</td>\n      <td>0.008622</td>\n      <td>0.022517</td>\n      <td>-0.040034</td>\n      <td>0.014473</td>\n      <td>0.011391</td>\n      <td>The Certificate Trust Policy component in Appl...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CVE-2011-0220</td>\n      <td>product-security@apple.com</td>\n      <td>2020-02-05 20:15:00+00:00</td>\n      <td>2020-02-07 19:24:00+00:00</td>\n      <td>3.6</td>\n      <td>LOCAL</td>\n      <td>LOW</td>\n      <td>['cpe:2.3:a:apple:bonjour:*:*:*:*:*:*:*:*']</td>\n      <td>['https://opensource.apple.com/source/mDNSResp...</td>\n      <td>2020</td>\n      <td>...</td>\n      <td>0.016330</td>\n      <td>0.049928</td>\n      <td>0.008830</td>\n      <td>0.000793</td>\n      <td>0.017700</td>\n      <td>0.028334</td>\n      <td>0.005227</td>\n      <td>0.049708</td>\n      <td>-0.015749</td>\n      <td>Apple Bonjour before 2011 allows a crash via a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CVE-2011-0428</td>\n      <td>cve@mitre.org</td>\n      <td>2019-10-29 19:15:00+00:00</td>\n      <td>2019-11-01 14:55:00+00:00</td>\n      <td>2.7</td>\n      <td>NETWORK</td>\n      <td>MEDIUM</td>\n      <td>['cpe:2.3:a:ikiwiki:ikiwiki:*:*:*:*:*:*:*:*']</td>\n      <td>['https://security-tracker.debian.org/tracker/...</td>\n      <td>2019</td>\n      <td>...</td>\n      <td>0.016057</td>\n      <td>-0.002453</td>\n      <td>0.037362</td>\n      <td>-0.007217</td>\n      <td>-0.034629</td>\n      <td>-0.005441</td>\n      <td>-0.000510</td>\n      <td>-0.015185</td>\n      <td>-0.025799</td>\n      <td>Cross Site Scripting (XSS) in ikiwiki before 3...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CVE-2011-0467</td>\n      <td>security@opentext.com</td>\n      <td>2018-06-07 21:29:00+00:00</td>\n      <td>2023-11-07 02:06:00+00:00</td>\n      <td>5.9</td>\n      <td>NETWORK</td>\n      <td>LOW</td>\n      <td>['cpe:2.3:a:suse:studio_onsite_appliance:*:*:*...</td>\n      <td>['https://bugzilla.suse.com/show_bug.cgi?id=67...</td>\n      <td>2018</td>\n      <td>...</td>\n      <td>0.023072</td>\n      <td>-0.052349</td>\n      <td>0.029655</td>\n      <td>-0.052779</td>\n      <td>-0.018734</td>\n      <td>-0.022419</td>\n      <td>-0.001663</td>\n      <td>0.004668</td>\n      <td>-0.070693</td>\n      <td>A vulnerability in the listing of available so...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CVE-2011-0469</td>\n      <td>security@opentext.com</td>\n      <td>2017-08-17 16:29:00+00:00</td>\n      <td>2023-11-07 02:06:00+00:00</td>\n      <td>5.9</td>\n      <td>NETWORK</td>\n      <td>LOW</td>\n      <td>['cpe:2.3:o:suse:opensuse:-:*:*:*:*:*:*:*']</td>\n      <td>['https://bugzilla.suse.com/show_bug.cgi?id=67...</td>\n      <td>2017</td>\n      <td>...</td>\n      <td>0.008663</td>\n      <td>0.009789</td>\n      <td>0.012439</td>\n      <td>-0.032353</td>\n      <td>0.007584</td>\n      <td>-0.005330</td>\n      <td>0.010918</td>\n      <td>-0.033823</td>\n      <td>0.007783</td>\n      <td>Code injection in openSUSE when running some s...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>107082</th>\n      <td>CVE-2008-7315</td>\n      <td>cve@mitre.org</td>\n      <td>2017-10-10 16:29:00+00:00</td>\n      <td>2017-11-03 17:15:00+00:00</td>\n      <td>5.9</td>\n      <td>NETWORK</td>\n      <td>LOW</td>\n      <td>['cpe:2.3:a:cpan:ui\\\\:\\\\:dialog:0.01:*:*:*:*:*...</td>\n      <td>['https://security-tracker.debian.org/tracker/...</td>\n      <td>2017</td>\n      <td>...</td>\n      <td>0.019123</td>\n      <td>-0.046788</td>\n      <td>0.001866</td>\n      <td>0.024574</td>\n      <td>-0.024319</td>\n      <td>-0.030054</td>\n      <td>0.025092</td>\n      <td>-0.023828</td>\n      <td>-0.022433</td>\n      <td>UI-Dialog 1.09 and earlier allows remote attac...</td>\n    </tr>\n    <tr>\n      <th>107083</th>\n      <td>CVE-2008-7316</td>\n      <td>security@debian.org</td>\n      <td>2016-05-02 10:59:00+00:00</td>\n      <td>2016-05-06 00:54:00+00:00</td>\n      <td>3.6</td>\n      <td>LOCAL</td>\n      <td>LOW</td>\n      <td>['cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*']</td>\n      <td>['http://git.kernel.org/cgit/linux/kernel/git/...</td>\n      <td>2016</td>\n      <td>...</td>\n      <td>0.014300</td>\n      <td>-0.012750</td>\n      <td>-0.036155</td>\n      <td>-0.043011</td>\n      <td>0.024458</td>\n      <td>0.012200</td>\n      <td>-0.009282</td>\n      <td>0.023590</td>\n      <td>-0.018185</td>\n      <td>mm/filemap.c in the Linux kernel before 2.6.25...</td>\n    </tr>\n    <tr>\n      <th>107084</th>\n      <td>CVE-2008-7319</td>\n      <td>cve@mitre.org</td>\n      <td>2017-11-07 21:29:00+00:00</td>\n      <td>2017-11-29 15:49:00+00:00</td>\n      <td>5.9</td>\n      <td>NETWORK</td>\n      <td>LOW</td>\n      <td>['cpe:2.3:a:net-ping-external_project:net-ping...</td>\n      <td>['https://rt.cpan.org/Public/Bug/Display.html?...</td>\n      <td>2017</td>\n      <td>...</td>\n      <td>0.034606</td>\n      <td>-0.056721</td>\n      <td>-0.028375</td>\n      <td>-0.023293</td>\n      <td>-0.014179</td>\n      <td>-0.005051</td>\n      <td>-0.029391</td>\n      <td>-0.020186</td>\n      <td>-0.033210</td>\n      <td>The Net::Ping::External extension through 0.15...</td>\n    </tr>\n    <tr>\n      <th>107085</th>\n      <td>CVE-2008-7320</td>\n      <td>cve@mitre.org</td>\n      <td>2018-11-18 19:29:00+00:00</td>\n      <td>2024-08-07 12:15:00+00:00</td>\n      <td>5.9</td>\n      <td>LOCAL</td>\n      <td>LOW</td>\n      <td>['cpe:2.3:a:gnome:seahorse:*:*:*:*:*:*:*:*']</td>\n      <td>['https://www.bountysource.com/issues/3849352-...</td>\n      <td>2018</td>\n      <td>...</td>\n      <td>-0.022035</td>\n      <td>0.005189</td>\n      <td>-0.003003</td>\n      <td>0.003738</td>\n      <td>0.012156</td>\n      <td>-0.027635</td>\n      <td>-0.001326</td>\n      <td>0.000663</td>\n      <td>-0.036381</td>\n      <td>GNOME Seahorse through 3.30 allows physically ...</td>\n    </tr>\n    <tr>\n      <th>107086</th>\n      <td>CVE-2008-7321</td>\n      <td>cve@mitre.org</td>\n      <td>2019-08-22 14:15:00+00:00</td>\n      <td>2019-08-23 19:44:00+00:00</td>\n      <td>2.7</td>\n      <td>NETWORK</td>\n      <td>MEDIUM</td>\n      <td>['cpe:2.3:a:tubepress:tubepress:*:*:*:*:*:word...</td>\n      <td>['https://wordpress.org/plugins/tubepress/#dev...</td>\n      <td>2019</td>\n      <td>...</td>\n      <td>-0.005189</td>\n      <td>-0.073709</td>\n      <td>0.005477</td>\n      <td>0.011733</td>\n      <td>-0.017345</td>\n      <td>-0.009489</td>\n      <td>-0.022760</td>\n      <td>0.000676</td>\n      <td>0.032018</td>\n      <td>The tubepress plugin before 1.6.5 for WordPres...</td>\n    </tr>\n  </tbody>\n</table>\n<p>107087 rows × 784 columns</p>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Impact_Score","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Assuming 'df' is your DataFrame with all the columns\n# Define columns to exclude\ncolumns_to_exclude = [\n    'Impact_Score', 'CVE_ID', 'ASSIGNER', 'Description', \n    'Published_Date', 'Last_Modified_Date', \n    'Access_Vector', 'Access_Complexity', \n    'Configurations', 'Reference_Data',\n    'Year', 'Base_Score', 'Exploitability_Score', \n    'Confidentiality_Impact', 'Integrity_Impact', \n    'Availability_Impact'\n]\n\n# Separate the target variable\ny = df['Impact_Score']  # Your target variable\n\n# Select all columns except the excluded ones\nX = df.drop(columns=columns_to_exclude)\n\n# Check shapes\nprint(X.shape)  # Should show (n_samples, n_features) where n_features should only include your embedding columns\nprint(y.shape)  # Should show (n_samples,)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:23:05.364860Z","iopub.execute_input":"2024-11-06T08:23:05.365537Z","iopub.status.idle":"2024-11-06T08:23:05.557966Z","shell.execute_reply.started":"2024-11-06T08:23:05.365496Z","shell.execute_reply":"2024-11-06T08:23:05.556981Z"}},"outputs":[{"name":"stdout","text":"(107087, 768)\n(107087,)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import pandas as pd\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n# Load your dataset\n# X, y = ... (Load your features and target variable)\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train XGBoost Regressor with GPU\nxgb_regressor_gpu = xgb.XGBRegressor(tree_method='gpu_hist', gpu_id=0)\nxgb_regressor_gpu.fit(X_train, y_train)\n\n# Predict and evaluate XGBoost\ny_pred_xgb_gpu = xgb_regressor_gpu.predict(X_test)\nmse_xgb_gpu = mean_squared_error(y_test, y_pred_xgb_gpu)\nmae_xgb_gpu = mean_absolute_error(y_test, y_pred_xgb_gpu)\nr2_xgb_gpu = r2_score(y_test, y_pred_xgb_gpu)\n\nprint(\"XGBoost GPU Metrics:\")\nprint(f\"MSE: {mse_xgb_gpu:.4f}, MAE: {mae_xgb_gpu:.4f}, R²: {r2_xgb_gpu:.4f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:23:06.001600Z","iopub.execute_input":"2024-11-06T08:23:06.002428Z","iopub.status.idle":"2024-11-06T08:23:16.730423Z","shell.execute_reply.started":"2024-11-06T08:23:06.002381Z","shell.execute_reply":"2024-11-06T08:23:16.729361Z"}},"outputs":[{"name":"stdout","text":"XGBoost GPU Metrics:\nMSE: 1.1454, MAE: 0.7909, R²: 0.4984\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n# Load your dataset\n# X, y = ... (Load your features and target variable)\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train LightGBM Regressor with GPU\nlgb_regressor_gpu = lgb.LGBMRegressor(device='gpu')\nlgb_regressor_gpu.fit(X_train, y_train)\n\n# Predict and evaluate LightGBM\ny_pred_lgb_gpu = lgb_regressor_gpu.predict(X_test)\nmse_lgb_gpu = mean_squared_error(y_test, y_pred_lgb_gpu)\nmae_lgb_gpu = mean_absolute_error(y_test, y_pred_lgb_gpu)\nr2_lgb_gpu = r2_score(y_test, y_pred_lgb_gpu)\n\nprint(\"LightGBM GPU Metrics:\")\nprint(f\"MSE: {mse_lgb_gpu:.4f}, MAE: {mae_lgb_gpu:.4f}, R²: {r2_lgb_gpu:.4f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:23:16.732034Z","iopub.execute_input":"2024-11-06T08:23:16.732416Z","iopub.status.idle":"2024-11-06T08:23:39.872085Z","shell.execute_reply.started":"2024-11-06T08:23:16.732381Z","shell.execute_reply":"2024-11-06T08:23:39.870450Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 195840\n[LightGBM] [Info] Number of data points in the train set: 85669, number of used features: 768\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 768 dense feature groups (62.75 MB) transferred to GPU in 0.052703 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score 4.379723\nLightGBM GPU Metrics:\nMSE: 1.1747, MAE: 0.8198, R²: 0.4856\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import pickle\n# Save the trained model using pickle\nmodel_filename = 'impact_Score_xgb_regressor.pkl'\nwith open(model_filename, 'wb') as file:\n    pickle.dump(xgb_regressor_gpu, file)\n\nprint(f\"Model saved as {model_filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:23:39.873255Z","iopub.execute_input":"2024-11-06T08:23:39.873605Z","iopub.status.idle":"2024-11-06T08:23:39.898484Z","shell.execute_reply.started":"2024-11-06T08:23:39.873567Z","shell.execute_reply":"2024-11-06T08:23:39.895844Z"}},"outputs":[{"name":"stdout","text":"Model saved as impact_Score_xgb_regressor.pkl\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import pandas as pd\nimport pickle\nimport numpy as np\n\n# Load the model\nwith open(\"impact_Score_xgb_regressor.pkl\", 'rb') as file:\n    loaded_model = pickle.load(file)\n\n# Ensure y_test is a 1D NumPy array or pandas Series\nif isinstance(y_test, pd.DataFrame):\n    y_test = y_test.squeeze()  # Convert DataFrame to Series if it's a single column\n\n# Select a random row from the test set\nrandom_index = np.random.randint(0, len(X_test))  # Select random index\nrandom_input = X_test.iloc[random_index].values.reshape(1, -1)  # Use .iloc for correct indexing\nreal_output = y_test.iloc[random_index] if isinstance(y_test, pd.Series) else y_test[random_index]  # Access real output using .iloc\n\n# Predict the output using the loaded model\npredicted_output = loaded_model.predict(random_input)\n\nprint(f\"Real Output: {real_output:.4f}\")\nprint(f\"Predicted Output: {predicted_output[0]:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:23:39.902969Z","iopub.execute_input":"2024-11-06T08:23:39.903293Z","iopub.status.idle":"2024-11-06T08:23:39.971590Z","shell.execute_reply.started":"2024-11-06T08:23:39.903259Z","shell.execute_reply":"2024-11-06T08:23:39.970845Z"}},"outputs":[{"name":"stdout","text":"Real Output: 5.9000\nPredicted Output: 5.5671\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BASE SCORE","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming 'df' is your DataFrame with all the columns\n# Define columns to exclude\ncolumns_to_exclude = [\n    'Impact_Score', 'CVE_ID', 'ASSIGNER', 'Description', \n    'Published_Date', 'Last_Modified_Date', \n    'Access_Vector', 'Access_Complexity', \n    'Configurations', 'Reference_Data',\n    'Year', 'Base_Score', 'Exploitability_Score', \n    'Confidentiality_Impact', 'Integrity_Impact', \n    'Availability_Impact'\n]\n\n# Separate the target variable\ny = df['Base_Score']  # Your target variable\n\n# Select all columns except the excluded ones\nX = df.drop(columns=columns_to_exclude)\n\n# Check shapes\nprint(X.shape)  # Should show (n_samples, n_features) where n_features should only include your embedding columns\nprint(y.shape)  # Should show (n_samples,)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:23:39.972855Z","iopub.execute_input":"2024-11-06T08:23:39.973419Z","iopub.status.idle":"2024-11-06T08:23:40.174933Z","shell.execute_reply.started":"2024-11-06T08:23:39.973383Z","shell.execute_reply":"2024-11-06T08:23:40.173824Z"}},"outputs":[{"name":"stdout","text":"(107087, 768)\n(107087,)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import pandas as pd\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n# Load your dataset\n# X, y = ... (Load your features and target variable)\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train XGBoost Regressor with GPU\nxgb_regressor_gpu = xgb.XGBRegressor(tree_method='gpu_hist', gpu_id=0)\nxgb_regressor_gpu.fit(X_train, y_train)\n\n# Predict and evaluate XGBoost\ny_pred_xgb_gpu = xgb_regressor_gpu.predict(X_test)\nmse_xgb_gpu = mean_squared_error(y_test, y_pred_xgb_gpu)\nmae_xgb_gpu = mean_absolute_error(y_test, y_pred_xgb_gpu)\nr2_xgb_gpu = r2_score(y_test, y_pred_xgb_gpu)\n\nprint(\"XGBoost GPU Metrics:\")\nprint(f\"MSE: {mse_xgb_gpu:.4f}, MAE: {mae_xgb_gpu:.4f}, R²: {r2_xgb_gpu:.4f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:23:40.176502Z","iopub.execute_input":"2024-11-06T08:23:40.176957Z","iopub.status.idle":"2024-11-06T08:23:51.182043Z","shell.execute_reply.started":"2024-11-06T08:23:40.176908Z","shell.execute_reply":"2024-11-06T08:23:51.180888Z"}},"outputs":[{"name":"stdout","text":"XGBoost GPU Metrics:\nMSE: 1.4874, MAE: 0.9358, R²: 0.4681\n\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import pandas as pd\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n# Load your dataset\n# X, y = ... (Load your features and target variable)\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train LightGBM Regressor with GPU\nlgb_regressor_gpu = lgb.LGBMRegressor(device='gpu')\nlgb_regressor_gpu.fit(X_train, y_train)\n\n# Predict and evaluate LightGBM\ny_pred_lgb_gpu = lgb_regressor_gpu.predict(X_test)\nmse_lgb_gpu = mean_squared_error(y_test, y_pred_lgb_gpu)\nmae_lgb_gpu = mean_absolute_error(y_test, y_pred_lgb_gpu)\nr2_lgb_gpu = r2_score(y_test, y_pred_lgb_gpu)\n\nprint(\"LightGBM GPU Metrics:\")\nprint(f\"MSE: {mse_lgb_gpu:.4f}, MAE: {mae_lgb_gpu:.4f}, R²: {r2_lgb_gpu:.4f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:23:51.183457Z","iopub.execute_input":"2024-11-06T08:23:51.183801Z","iopub.status.idle":"2024-11-06T08:24:14.293789Z","shell.execute_reply.started":"2024-11-06T08:23:51.183764Z","shell.execute_reply":"2024-11-06T08:24:14.292486Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 195840\n[LightGBM] [Info] Number of data points in the train set: 85669, number of used features: 768\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 768 dense feature groups (62.75 MB) transferred to GPU in 0.049063 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score 6.439187\nLightGBM GPU Metrics:\nMSE: 1.5338, MAE: 0.9703, R²: 0.4515\n\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import pickle\n# Save the trained model using pickle\nmodel_filename = 'base_score_xgb_regressor.pkl'\nwith open(model_filename, 'wb') as file:\n    pickle.dump(xgb_regressor_gpu, file)\n\nprint(f\"Model saved as {model_filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:24:14.295697Z","iopub.execute_input":"2024-11-06T08:24:14.296326Z","iopub.status.idle":"2024-11-06T08:24:14.308161Z","shell.execute_reply.started":"2024-11-06T08:24:14.296284Z","shell.execute_reply":"2024-11-06T08:24:14.307314Z"}},"outputs":[{"name":"stdout","text":"Model saved as base_score_xgb_regressor.pkl\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import pandas as pd\nimport pickle\nimport numpy as np\n\n# Load the model\nwith open(\"base_score_xgb_regressor.pkl\", 'rb') as file:\n    loaded_model = pickle.load(file)\n\n# Ensure y_test has a reset index if necessary\ny_test = y_test.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)\n\n# Select a random row from the test set\nrandom_index = np.random.randint(0, len(X_test))  # Select random index\nrandom_input = X_test.iloc[random_index].values.reshape(1, -1)  # Use .iloc and reshape for prediction\nreal_output = y_test.iloc[random_index]  # Access real output using .iloc\n\n# Predict the output using the loaded model\npredicted_output = loaded_model.predict(random_input)\n\n# Display the results\nprint(f\"Real Output: {real_output:.4f}\")\nprint(f\"Predicted Output: {predicted_output[0]:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:24:14.309345Z","iopub.execute_input":"2024-11-06T08:24:14.319049Z","iopub.status.idle":"2024-11-06T08:24:14.407982Z","shell.execute_reply.started":"2024-11-06T08:24:14.318996Z","shell.execute_reply":"2024-11-06T08:24:14.407035Z"}},"outputs":[{"name":"stdout","text":"Real Output: 9.9000\nPredicted Output: 7.7873\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploitability Score","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming 'df' is your DataFrame with all the columns\n# Define columns to exclude\ncolumns_to_exclude = [\n    'Impact_Score', 'CVE_ID', 'ASSIGNER', 'Description', \n    'Published_Date', 'Last_Modified_Date', \n    'Access_Vector', 'Access_Complexity', \n    'Configurations', 'Reference_Data',\n    'Year', 'Base_Score', 'Exploitability_Score', \n    'Confidentiality_Impact', 'Integrity_Impact', \n    'Availability_Impact'\n]\n\n# Separate the target variable\ny = df['Exploitability_Score']  # Your target variable\n\n# Select all columns except the excluded ones\nX = df.drop(columns=columns_to_exclude)\n\n# Check shapes\nprint(X.shape)  # Should show (n_samples, n_features) where n_features should only include your embedding columns\nprint(y.shape)  # Should show (n_samples,)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:24:14.411133Z","iopub.execute_input":"2024-11-06T08:24:14.412163Z","iopub.status.idle":"2024-11-06T08:24:14.620765Z","shell.execute_reply.started":"2024-11-06T08:24:14.412096Z","shell.execute_reply":"2024-11-06T08:24:14.619701Z"}},"outputs":[{"name":"stdout","text":"(107087, 768)\n(107087,)\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import pandas as pd\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n# Load your dataset\n# X, y = ... (Load your features and target variable)\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train XGBoost Regressor with GPU\nxgb_regressor_gpu = xgb.XGBRegressor(tree_method='gpu_hist', gpu_id=0)\nxgb_regressor_gpu.fit(X_train, y_train)\n\n# Predict and evaluate XGBoost\ny_pred_xgb_gpu = xgb_regressor_gpu.predict(X_test)\nmse_xgb_gpu = mean_squared_error(y_test, y_pred_xgb_gpu)\nmae_xgb_gpu = mean_absolute_error(y_test, y_pred_xgb_gpu)\nr2_xgb_gpu = r2_score(y_test, y_pred_xgb_gpu)\n\nprint(\"XGBoost GPU Metrics:\")\nprint(f\"MSE: {mse_xgb_gpu:.4f}, MAE: {mae_xgb_gpu:.4f}, R²: {r2_xgb_gpu:.4f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:24:14.621917Z","iopub.execute_input":"2024-11-06T08:24:14.622240Z","iopub.status.idle":"2024-11-06T08:24:25.275535Z","shell.execute_reply.started":"2024-11-06T08:24:14.622207Z","shell.execute_reply":"2024-11-06T08:24:25.274485Z"}},"outputs":[{"name":"stdout","text":"XGBoost GPU Metrics:\nMSE: 1.1731, MAE: 0.8116, R²: 0.4526\n\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"import pandas as pd\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n# Load your dataset\n# X, y = ... (Load your features and target variable)\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train LightGBM Regressor with GPU\nlgb_regressor_gpu = lgb.LGBMRegressor(device='gpu')\nlgb_regressor_gpu.fit(X_train, y_train)\n\n# Predict and evaluate LightGBM\ny_pred_lgb_gpu = lgb_regressor_gpu.predict(X_test)\nmse_lgb_gpu = mean_squared_error(y_test, y_pred_lgb_gpu)\nmae_lgb_gpu = mean_absolute_error(y_test, y_pred_lgb_gpu)\nr2_lgb_gpu = r2_score(y_test, y_pred_lgb_gpu)\n\nprint(\"LightGBM GPU Metrics:\")\nprint(f\"MSE: {mse_lgb_gpu:.4f}, MAE: {mae_lgb_gpu:.4f}, R²: {r2_lgb_gpu:.4f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:24:25.276751Z","iopub.execute_input":"2024-11-06T08:24:25.277048Z","iopub.status.idle":"2024-11-06T08:24:47.920898Z","shell.execute_reply.started":"2024-11-06T08:24:25.277016Z","shell.execute_reply":"2024-11-06T08:24:47.919825Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 195840\n[LightGBM] [Info] Number of data points in the train set: 85669, number of used features: 768\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 768 dense feature groups (62.75 MB) transferred to GPU in 0.105714 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score 5.308553\nLightGBM GPU Metrics:\nMSE: 1.2120, MAE: 0.8494, R²: 0.4344\n\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# Save the trained model using pickle\nmodel_filename = 'exploitability_Score_xgb_regressor.pkl'\nwith open(model_filename, 'wb') as file:\n    pickle.dump(xgb_regressor_gpu, file)\n\nprint(f\"Model saved as {model_filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:24:47.922480Z","iopub.execute_input":"2024-11-06T08:24:47.922876Z","iopub.status.idle":"2024-11-06T08:24:47.939841Z","shell.execute_reply.started":"2024-11-06T08:24:47.922837Z","shell.execute_reply":"2024-11-06T08:24:47.937706Z"}},"outputs":[{"name":"stdout","text":"Model saved as exploitability_Score_xgb_regressor.pkl\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"import pandas as pd\nimport pickle\nimport numpy as np\n\n# Load the model\nwith open(\"exploitability_Score_xgb_regressor.pkl\", 'rb') as file:\n    loaded_model = pickle.load(file)\n\n# Reset the index of y_test to ensure it matches with X_test\ny_test = y_test.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)  # Reset X_test index to ensure alignment\n\n# Select a random row from the test set\nrandom_index = np.random.randint(0, len(X_test))  # Select random index\nrandom_input = X_test.iloc[random_index].values.reshape(1, -1)  # Use .iloc for positional indexing\nreal_output = y_test.iloc[random_index]  # Access real output using .iloc\n\n# Predict the output using the loaded model\npredicted_output = loaded_model.predict(random_input)\n\n# Display the results\nprint(f\"Real Output: {real_output:.4f}\")\nprint(f\"Predicted Output: {predicted_output[0]:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:24:47.941349Z","iopub.execute_input":"2024-11-06T08:24:47.941945Z","iopub.status.idle":"2024-11-06T08:24:48.025523Z","shell.execute_reply.started":"2024-11-06T08:24:47.941903Z","shell.execute_reply":"2024-11-06T08:24:48.024482Z"}},"outputs":[{"name":"stdout","text":"Real Output: 5.7000\nPredicted Output: 5.7402\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ACCESS COMPLEXITY","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming 'df' is your DataFrame with all the columns\n# Define columns to exclude\ncolumns_to_exclude = [\n    'Impact_Score', 'CVE_ID', 'ASSIGNER', 'Description', \n    'Published_Date', 'Last_Modified_Date', \n    'Access_Vector', 'Access_Complexity', \n    'Configurations', 'Reference_Data',\n    'Year', 'Base_Score', 'Exploitability_Score', \n    'Confidentiality_Impact', 'Integrity_Impact', \n    'Availability_Impact'\n]\n\n# Separate the target variable\ny = df['Access_Complexity']  # Your target variable\n\n# Select all columns except the excluded ones\nX = df.drop(columns=columns_to_exclude)\n\n# Check shapes\nprint(X.shape)  # Should show (n_samples, n_features) where n_features should only include your embedding columns\nprint(y.shape)  # Should show (n_samples,)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:24:54.267642Z","iopub.execute_input":"2024-11-06T08:24:54.268029Z","iopub.status.idle":"2024-11-06T08:24:54.470581Z","shell.execute_reply.started":"2024-11-06T08:24:54.267992Z","shell.execute_reply":"2024-11-06T08:24:54.469277Z"}},"outputs":[{"name":"stdout","text":"(107087, 768)\n(107087,)\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error\nimport pickle\n\n# Encode categorical labels into numeric values\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# Save the LabelEncoder as a pickle file\nwith open(\"Access_Complexity_label_encoder.pkl\", 'wb') as f:\n    pickle.dump(label_encoder, f)\n\n# Variables to store the best model and accuracy\nbest_model = None\nbest_accuracy = 0\nbest_model_name = \"\"\n\n# Function to train and evaluate a model\ndef train_and_evaluate(model, model_name):\n    global best_model, best_accuracy, best_model_name\n    \n    # Fit the model\n    model.fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_test, predictions)\n    mse = mean_squared_error(y_test, predictions)\n    mae = mean_absolute_error(y_test, predictions)\n\n    print(f\"{model_name} Accuracy: {accuracy:.4f}\")\n    print(f\"{model_name} Mean Squared Error: {mse:.4f}\")\n    print(f\"{model_name} Mean Absolute Error: {mae:.4f}\\n\")\n\n    # Update the best model if the current model has a higher accuracy\n    if accuracy > best_accuracy:\n        best_accuracy = accuracy\n        best_model = model\n        best_model_name = model_name\n\n# Initialize models with GPU support\nmodels = {\n    'XGBoost': XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor', eval_metric='mlogloss', random_state=42),\n    'CatBoost': CatBoostClassifier(task_type='GPU', verbose=0, random_state=42),\n    'LightGBM': LGBMClassifier(device='gpu', random_state=42)\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    train_and_evaluate(model, name)\n\n# Save the best model as a pickle file\nif best_model:\n    with open(f'Access_Complexity_best_{best_model_name.lower().replace(\" \", \"_\")}_model.pkl', 'wb') as f:\n        pickle.dump(best_model, f)\n    print(f\"Best model ({best_model_name}) saved with accuracy: {best_accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:26:09.452173Z","iopub.execute_input":"2024-11-06T08:26:09.452891Z","iopub.status.idle":"2024-11-06T08:27:35.911395Z","shell.execute_reply.started":"2024-11-06T08:26:09.452851Z","shell.execute_reply":"2024-11-06T08:27:35.910247Z"}},"outputs":[{"name":"stdout","text":"XGBoost Accuracy: 0.8496\nXGBoost Mean Squared Error: 0.1677\nXGBoost Mean Absolute Error: 0.1562\n\nCatBoost Accuracy: 0.8517\nCatBoost Mean Squared Error: 0.1670\nCatBoost Mean Absolute Error: 0.1545\n\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 195840\n[LightGBM] [Info] Number of data points in the train set: 85669, number of used features: 768\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n","output_type":"stream"},{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 768 dense feature groups (62.75 MB) transferred to GPU in 0.049669 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score -3.974878\n[LightGBM] [Info] Start training from score -0.544687\n[LightGBM] [Info] Start training from score -0.913307\nLightGBM Accuracy: 0.8383\nLightGBM Mean Squared Error: 0.1787\nLightGBM Mean Absolute Error: 0.1674\n\nBest model (CatBoost) saved with accuracy: 0.8517\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"import pickle\nimport numpy as np\n\n# Load the saved model and label encoder\nwith open(\"Access_Complexity_label_encoder.pkl\", 'rb') as f:\n    label_encoder = pickle.load(f)\n\nwith open(\"Access_Complexity_best_catboost_model.pkl\", 'rb') as f:\n    best_model = pickle.load(f)\n\n# Select a random sample from the test set\nrandom_index = np.random.randint(0, X_test.shape[0])\nrandom_sample = X_test[random_index:random_index + 1]\n\n# Predict the label for the sample (CatBoost requires data on CPU)\npredicted_label_encoded = best_model.predict(random_sample)\n\n# Convert the encoded prediction back to the original label\npredicted_label = label_encoder.inverse_transform(predicted_label_encoded.astype(int))\n\n# Get the actual label for the sample\nactual_label_encoded = y_test[random_index]\nactual_label = label_encoder.inverse_transform([actual_label_encoded])\n\n# Display the actual and predicted values\nprint(f\"Actual label: {actual_label[0]}\")\nprint(f\"Predicted label: {predicted_label[0]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:28:00.847746Z","iopub.execute_input":"2024-11-06T08:28:00.849026Z","iopub.status.idle":"2024-11-06T08:28:00.910753Z","shell.execute_reply.started":"2024-11-06T08:28:00.848939Z","shell.execute_reply":"2024-11-06T08:28:00.909616Z"}},"outputs":[{"name":"stdout","text":"Actual label: LOW\nPredicted label: LOW\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"# Access Vector","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming 'df' is your DataFrame with all the columns\n# Define columns to exclude\ncolumns_to_exclude = [\n    'Impact_Score', 'CVE_ID', 'ASSIGNER', 'Description', \n    'Published_Date', 'Last_Modified_Date', \n    'Access_Vector', 'Access_Complexity', \n    'Configurations', 'Reference_Data',\n    'Year', 'Base_Score', 'Exploitability_Score', \n    'Confidentiality_Impact', 'Integrity_Impact', \n    'Availability_Impact'\n]\n\n# Separate the target variable\ny = df['Access_Vector']  # Your target variable\n\n# Select all columns except the excluded ones\nX = df.drop(columns=columns_to_exclude)\n\n# Check shapes\nprint(X.shape)  # Should show (n_samples, n_features) where n_features should only include your embedding columns\nprint(y.shape)  # Should show (n_samples,)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:28:04.896864Z","iopub.execute_input":"2024-11-06T08:28:04.897369Z","iopub.status.idle":"2024-11-06T08:28:05.091688Z","shell.execute_reply.started":"2024-11-06T08:28:04.897332Z","shell.execute_reply":"2024-11-06T08:28:05.090749Z"}},"outputs":[{"name":"stdout","text":"(107087, 768)\n(107087,)\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error\nimport pickle\n\n# Encode categorical labels into numeric values\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# Save the LabelEncoder as a pickle file\nwith open(\"Access_Vector_label_encoder.pkl\", 'wb') as f:\n    pickle.dump(label_encoder, f)\n\n# Variables to store the best model and accuracy\nbest_model = None\nbest_accuracy = 0\nbest_model_name = \"\"\n\n# Function to train and evaluate a model\ndef train_and_evaluate(model, model_name):\n    global best_model, best_accuracy, best_model_name\n    \n    # Fit the model\n    model.fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_test, predictions)\n    mse = mean_squared_error(y_test, predictions)\n    mae = mean_absolute_error(y_test, predictions)\n\n    print(f\"{model_name} Accuracy: {accuracy:.4f}\")\n    print(f\"{model_name} Mean Squared Error: {mse:.4f}\")\n    print(f\"{model_name} Mean Absolute Error: {mae:.4f}\\n\")\n\n    # Update the best model if the current model has a higher accuracy\n    if accuracy > best_accuracy:\n        best_accuracy = accuracy\n        best_model = model\n        best_model_name = model_name\n\n# Initialize models with GPU support\nmodels = {\n    'XGBoost': XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor', eval_metric='mlogloss', random_state=42),\n    'CatBoost': CatBoostClassifier(task_type='GPU', verbose=0, random_state=42),\n    'LightGBM': LGBMClassifier(device='gpu', random_state=42)\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    train_and_evaluate(model, name)\n\n# Save the best model as a pickle file\nif best_model:\n    with open(f'accessVector_best_{best_model_name.lower().replace(\" \", \"_\")}_model.pkl', 'wb') as f:\n        pickle.dump(best_model, f)\n    print(f\"Best model ({best_model_name}) saved with accuracy: {best_accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:30:12.413281Z","iopub.execute_input":"2024-11-06T08:30:12.414062Z","iopub.status.idle":"2024-11-06T08:31:31.490243Z","shell.execute_reply.started":"2024-11-06T08:30:12.414017Z","shell.execute_reply":"2024-11-06T08:31:31.488015Z"}},"outputs":[{"name":"stdout","text":"XGBoost Accuracy: 0.9150\nXGBoost Mean Squared Error: 0.1281\nXGBoost Mean Absolute Error: 0.0994\n\nCatBoost Accuracy: 0.9153\nCatBoost Mean Squared Error: 0.1271\nCatBoost Mean Absolute Error: 0.0988\n\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 195840\n[LightGBM] [Info] Number of data points in the train set: 85669, number of used features: 768\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 768 dense feature groups (62.75 MB) transferred to GPU in 0.052736 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score -3.818156\n[LightGBM] [Info] Start training from score -1.835068\n[LightGBM] [Info] Start training from score -0.200369\nLightGBM Accuracy: 0.9099\nLightGBM Mean Squared Error: 0.1333\nLightGBM Mean Absolute Error: 0.1045\n\nBest model (CatBoost) saved with accuracy: 0.9153\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"import pickle\nimport numpy as np\n\n# Load the saved model and label encoder\nwith open(\"Access_Vector_label_encoder.pkl\", 'rb') as f:\n    label_encoder = pickle.load(f)\n\nwith open(\"accessVector_best_catboost_model.pkl\", 'rb') as f:\n    best_model = pickle.load(f)\n\n# Select a random sample from the test set\nrandom_index = np.random.randint(0, X_test.shape[0])\nrandom_sample = X_test[random_index:random_index + 1]\n\n# Predict the label for the sample (CatBoost requires data on CPU)\npredicted_label_encoded = best_model.predict(random_sample)\n\n# Convert the encoded prediction back to the original label\npredicted_label = label_encoder.inverse_transform(predicted_label_encoded.astype(int))\n\n# Get the actual label for the sample\nactual_label_encoded = y_test[random_index]\nactual_label = label_encoder.inverse_transform([actual_label_encoded])\n\n# Display the actual and predicted values\nprint(f\"Actual label: {actual_label[0]}\")\nprint(f\"Predicted label: {predicted_label[0]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:31:43.220002Z","iopub.execute_input":"2024-11-06T08:31:43.220429Z","iopub.status.idle":"2024-11-06T08:31:43.278881Z","shell.execute_reply.started":"2024-11-06T08:31:43.220364Z","shell.execute_reply":"2024-11-06T08:31:43.277936Z"}},"outputs":[{"name":"stdout","text":"Actual label: LOCAL\nPredicted label: LOCAL\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Availability Impact","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming 'df' is your DataFrame with all the columns\n# Define columns to exclude\ncolumns_to_exclude = [\n    'Impact_Score', 'CVE_ID', 'ASSIGNER', 'Description', \n    'Published_Date', 'Last_Modified_Date', \n    'Access_Vector', 'Access_Complexity', \n    'Configurations', 'Reference_Data',\n    'Year', 'Base_Score', 'Exploitability_Score', \n    'Confidentiality_Impact', 'Integrity_Impact', \n    'Availability_Impact'\n]\n\n# Separate the target variable\ny = df['Availability_Impact']  # Your target variable\n\n# Select all columns except the excluded ones\nX = df.drop(columns=columns_to_exclude)\n\n# Check shapes\nprint(X.shape)  # Should show (n_samples, n_features) where n_features should only include your embedding columns\nprint(y.shape)  # Should show (n_samples,)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:31:47.273991Z","iopub.execute_input":"2024-11-06T08:31:47.274422Z","iopub.status.idle":"2024-11-06T08:31:47.473124Z","shell.execute_reply.started":"2024-11-06T08:31:47.274384Z","shell.execute_reply":"2024-11-06T08:31:47.472065Z"}},"outputs":[{"name":"stdout","text":"(107087, 768)\n(107087,)\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error\nimport pickle\n\n# Encode categorical labels into numeric values\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# Save the LabelEncoder as a pickle file\nwith open(\"Availability_Impact_label_encoder.pkl\", 'wb') as f:\n    pickle.dump(label_encoder, f)\n\n# Variables to store the best model and accuracy\nbest_model = None\nbest_accuracy = 0\nbest_model_name = \"\"\n\n# Function to train and evaluate a model\ndef train_and_evaluate(model, model_name):\n    global best_model, best_accuracy, best_model_name\n    \n    # Fit the model\n    model.fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_test, predictions)\n    mse = mean_squared_error(y_test, predictions)\n    mae = mean_absolute_error(y_test, predictions)\n\n    print(f\"{model_name} Accuracy: {accuracy:.4f}\")\n    print(f\"{model_name} Mean Squared Error: {mse:.4f}\")\n    print(f\"{model_name} Mean Absolute Error: {mae:.4f}\\n\")\n\n    # Update the best model if the current model has a higher accuracy\n    if accuracy > best_accuracy:\n        best_accuracy = accuracy\n        best_model = model\n        best_model_name = model_name\n\n# Initialize models with GPU support\nmodels = {\n    'XGBoost': XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor', eval_metric='mlogloss', random_state=42),\n    'CatBoost': CatBoostClassifier(task_type='GPU', verbose=0, random_state=42),\n    'LightGBM': LGBMClassifier(device='gpu', random_state=42)\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    train_and_evaluate(model, name)\n\n# Save the best model as a pickle file\nif best_model:\n    with open(f'Availability_Impact_best_{best_model_name.lower().replace(\" \", \"_\")}_model.pkl', 'wb') as f:\n        pickle.dump(best_model, f)\n    print(f\"Best model ({best_model_name}) saved with accuracy: {best_accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:32:37.083421Z","iopub.execute_input":"2024-11-06T08:32:37.084226Z","iopub.status.idle":"2024-11-06T08:33:57.136882Z","shell.execute_reply.started":"2024-11-06T08:32:37.084187Z","shell.execute_reply":"2024-11-06T08:33:57.135350Z"}},"outputs":[{"name":"stdout","text":"XGBoost Accuracy: 0.8716\nXGBoost Mean Squared Error: 0.4591\nXGBoost Mean Absolute Error: 0.2387\n\nCatBoost Accuracy: 0.8729\nCatBoost Mean Squared Error: 0.4511\nCatBoost Mean Absolute Error: 0.2351\n\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 195840\n[LightGBM] [Info] Number of data points in the train set: 85669, number of used features: 768\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 768 dense feature groups (62.75 MB) transferred to GPU in 0.051445 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score -0.535512\n[LightGBM] [Info] Start training from score -3.691556\n[LightGBM] [Info] Start training from score -0.942384\nLightGBM Accuracy: 0.8644\nLightGBM Mean Squared Error: 0.4850\nLightGBM Mean Absolute Error: 0.2521\n\nBest model (CatBoost) saved with accuracy: 0.8729\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"import pickle\nimport numpy as np\n\n# Load the saved model and label encoder\nwith open(\"Availability_Impact_label_encoder.pkl\", 'rb') as f:\n    label_encoder = pickle.load(f)\n\nwith open(\"Availability_Impact_best_catboost_model.pkl\", 'rb') as f:\n    best_model = pickle.load(f)\n\n# Select a random sample from the test set\nrandom_index = np.random.randint(0, X_test.shape[0])\nrandom_sample = X_test[random_index:random_index + 1]\n\n# Predict the label for the sample (CatBoost requires data on CPU)\npredicted_label_encoded = best_model.predict(random_sample)\n\n# Convert the encoded prediction back to the original label\npredicted_label = label_encoder.inverse_transform(predicted_label_encoded.astype(int))\n\n# Get the actual label for the sample\nactual_label_encoded = y_test[random_index]\nactual_label = label_encoder.inverse_transform([actual_label_encoded])\n\n# Display the actual and predicted values\nprint(f\"Actual label: {actual_label[0]}\")\nprint(f\"Predicted label: {predicted_label[0]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:33:57.139511Z","iopub.execute_input":"2024-11-06T08:33:57.140456Z","iopub.status.idle":"2024-11-06T08:33:57.211291Z","shell.execute_reply.started":"2024-11-06T08:33:57.140381Z","shell.execute_reply":"2024-11-06T08:33:57.210060Z"}},"outputs":[{"name":"stdout","text":"Actual label: NONE\nPredicted label: NONE\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Confidentiality Impact","metadata":{}},{"cell_type":"code","source":"\nimport pandas as pd\n\n# Assuming 'df' is your DataFrame with all the columns\n# Define columns to exclude\ncolumns_to_exclude = [\n    'Impact_Score', 'CVE_ID', 'ASSIGNER', 'Description', \n    'Published_Date', 'Last_Modified_Date', \n    'Access_Vector', 'Access_Complexity', \n    'Configurations', 'Reference_Data',\n    'Year', 'Base_Score', 'Exploitability_Score', \n    'Confidentiality_Impact', 'Integrity_Impact', \n    'Availability_Impact'\n]\n\n# Separate the target variable\ny = df['Confidentiality_Impact']  # Your target variable\n\n# Select all columns except the excluded ones\nX = df.drop(columns=columns_to_exclude)\n\n# Check shapes\nprint(X.shape)  # Should show (n_samples, n_features) where n_features should only include your embedding columns\nprint(y.shape)  # Should show (n_samples,)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:33:57.213187Z","iopub.execute_input":"2024-11-06T08:33:57.214023Z","iopub.status.idle":"2024-11-06T08:33:57.423553Z","shell.execute_reply.started":"2024-11-06T08:33:57.213966Z","shell.execute_reply":"2024-11-06T08:33:57.422529Z"}},"outputs":[{"name":"stdout","text":"(107087, 768)\n(107087,)\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error\nimport pickle\n\n# Encode categorical labels into numeric values\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# Save the LabelEncoder as a pickle file\nwith open(\"Confidentiality_Impact_label_encoder.pkl\", 'wb') as f:\n    pickle.dump(label_encoder, f)\n\n# Variables to store the best model and accuracy\nbest_model = None\nbest_accuracy = 0\nbest_model_name = \"\"\n\n# Function to train and evaluate a model\ndef train_and_evaluate(model, model_name):\n    global best_model, best_accuracy, best_model_name\n    \n    # Fit the model\n    model.fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_test, predictions)\n    mse = mean_squared_error(y_test, predictions)\n    mae = mean_absolute_error(y_test, predictions)\n\n    print(f\"{model_name} Accuracy: {accuracy:.4f}\")\n    print(f\"{model_name} Mean Squared Error: {mse:.4f}\")\n    print(f\"{model_name} Mean Absolute Error: {mae:.4f}\\n\")\n\n    # Update the best model if the current model has a higher accuracy\n    if accuracy > best_accuracy:\n        best_accuracy = accuracy\n        best_model = model\n        best_model_name = model_name\n\n# Initialize models with GPU support\nmodels = {\n    'XGBoost': XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor', eval_metric='mlogloss', random_state=42),\n    'CatBoost': CatBoostClassifier(task_type='GPU', verbose=0, random_state=42),\n    'LightGBM': LGBMClassifier(device='gpu', random_state=42)\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    train_and_evaluate(model, name)\n\n# Save the best model as a pickle file\nif best_model:\n    with open(f'Confidentiality_Impact_best_{best_model_name.lower().replace(\" \", \"_\")}_model.pkl', 'wb') as f:\n        pickle.dump(best_model, f)\n    print(f\"Best model ({best_model_name}) saved with accuracy: {best_accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:33:57.425552Z","iopub.execute_input":"2024-11-06T08:33:57.425882Z","iopub.status.idle":"2024-11-06T08:35:19.659830Z","shell.execute_reply.started":"2024-11-06T08:33:57.425848Z","shell.execute_reply":"2024-11-06T08:35:19.658781Z"}},"outputs":[{"name":"stdout","text":"XGBoost Accuracy: 0.8406\nXGBoost Mean Squared Error: 0.4184\nXGBoost Mean Absolute Error: 0.2458\n\nCatBoost Accuracy: 0.8388\nCatBoost Mean Squared Error: 0.4199\nCatBoost Mean Absolute Error: 0.2475\n\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 195840\n[LightGBM] [Info] Number of data points in the train set: 85669, number of used features: 768\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 768 dense feature groups (62.75 MB) transferred to GPU in 0.048885 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score -0.531989\n[LightGBM] [Info] Start training from score -1.638081\n[LightGBM] [Info] Start training from score -1.522288\nLightGBM Accuracy: 0.8240\nLightGBM Mean Squared Error: 0.4666\nLightGBM Mean Absolute Error: 0.2729\n\nBest model (XGBoost) saved with accuracy: 0.8406\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"import pickle\nimport numpy as np\n\n# Load the saved model and label encoder\nwith open(\"Confidentiality_Impact_label_encoder.pkl\", 'rb') as f:\n    label_encoder = pickle.load(f)\n\nwith open(\"Confidentiality_Impact_best_xgboost_model.pkl\", 'rb') as f:\n    best_model = pickle.load(f)\n\n# Select a random sample from the test set\nrandom_index = np.random.randint(0, X_test.shape[0])\nrandom_sample = X_test[random_index:random_index + 1]\n\n# Predict the label for the sample (CatBoost requires data on CPU)\npredicted_label_encoded = best_model.predict(random_sample)\n\n# Convert the encoded prediction back to the original label\npredicted_label = label_encoder.inverse_transform(predicted_label_encoded.astype(int))\n\n# Get the actual label for the sample\nactual_label_encoded = y_test[random_index]\nactual_label = label_encoder.inverse_transform([actual_label_encoded])\n\n# Display the actual and predicted values\nprint(f\"Actual label: {actual_label[0]}\")\nprint(f\"Predicted label: {predicted_label[0]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:38:43.962148Z","iopub.execute_input":"2024-11-06T08:38:43.963044Z","iopub.status.idle":"2024-11-06T08:38:44.093834Z","shell.execute_reply.started":"2024-11-06T08:38:43.963005Z","shell.execute_reply":"2024-11-06T08:38:44.093103Z"}},"outputs":[{"name":"stdout","text":"Actual label: HIGH\nPredicted label: HIGH\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Integrity Impact","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming 'df' is your DataFrame with all the columns\n# Define columns to exclude\ncolumns_to_exclude = [\n    'Impact_Score', 'CVE_ID', 'ASSIGNER', 'Description', \n    'Published_Date', 'Last_Modified_Date', \n    'Access_Vector', 'Access_Complexity', \n    'Configurations', 'Reference_Data',\n    'Year', 'Base_Score', 'Exploitability_Score', \n    'Confidentiality_Impact', 'Integrity_Impact', \n    'Availability_Impact'\n]\n\n# Separate the target variable\ny = df['Integrity_Impact']  # Your target variable\n\n# Select all columns except the excluded ones\nX = df.drop(columns=columns_to_exclude)\n\n# Check shapes\nprint(X.shape)  # Should show (n_samples, n_features) where n_features should only include your embedding columns\nprint(y.shape)  # Should show (n_samples,)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:35:20.366834Z","iopub.status.idle":"2024-11-06T08:35:20.367270Z","shell.execute_reply.started":"2024-11-06T08:35:20.367054Z","shell.execute_reply":"2024-11-06T08:35:20.367075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error\nimport pickle\n\n# Encode categorical labels into numeric values\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# Save the LabelEncoder as a pickle file\nwith open(\"Integrity_Impact_label_encoder.pkl\", 'wb') as f:\n    pickle.dump(label_encoder, f)\n\n# Variables to store the best model and accuracy\nbest_model = None\nbest_accuracy = 0\nbest_model_name = \"\"\n\n# Function to train and evaluate a model\ndef train_and_evaluate(model, model_name):\n    global best_model, best_accuracy, best_model_name\n    \n    # Fit the model\n    model.fit(X_train, y_train)\n\n    predictions = model.predict(X_test)\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_test, predictions)\n    mse = mean_squared_error(y_test, predictions)\n    mae = mean_absolute_error(y_test, predictions)\n\n    print(f\"{model_name} Accuracy: {accuracy:.4f}\")\n    print(f\"{model_name} Mean Squared Error: {mse:.4f}\")\n    print(f\"{model_name} Mean Absolute Error: {mae:.4f}\\n\")\n\n    # Update the best model if the current model has a higher accuracy\n    if accuracy > best_accuracy:\n        best_accuracy = accuracy\n        best_model = model\n        best_model_name = model_name\n\n# Initialize models with GPU support\nmodels = {\n    'XGBoost': XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor', eval_metric='mlogloss', random_state=42),\n    'CatBoost': CatBoostClassifier(task_type='GPU', verbose=0, random_state=42),\n    'LightGBM': LGBMClassifier(device='gpu', random_state=42)\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    train_and_evaluate(model, name)\n\n# Save the best model as a pickle file\nif best_model:\n    with open(f'Integrity_Impact_best_{best_model_name.lower().replace(\" \", \"_\")}_model.pkl', 'wb') as f:\n        pickle.dump(best_model, f)\n    print(f\"Best model ({best_model_name}) saved with accuracy: {best_accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:35:33.841747Z","iopub.execute_input":"2024-11-06T08:35:33.842556Z","iopub.status.idle":"2024-11-06T08:36:54.433954Z","shell.execute_reply.started":"2024-11-06T08:35:33.842511Z","shell.execute_reply":"2024-11-06T08:36:54.432926Z"}},"outputs":[{"name":"stdout","text":"XGBoost Accuracy: 0.8406\nXGBoost Mean Squared Error: 0.4184\nXGBoost Mean Absolute Error: 0.2458\n\nCatBoost Accuracy: 0.8388\nCatBoost Mean Squared Error: 0.4199\nCatBoost Mean Absolute Error: 0.2475\n\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 195840\n[LightGBM] [Info] Number of data points in the train set: 85669, number of used features: 768\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 768 dense feature groups (62.75 MB) transferred to GPU in 0.051238 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score -0.531989\n[LightGBM] [Info] Start training from score -1.638081\n[LightGBM] [Info] Start training from score -1.522288\nLightGBM Accuracy: 0.8240\nLightGBM Mean Squared Error: 0.4666\nLightGBM Mean Absolute Error: 0.2729\n\nBest model (XGBoost) saved with accuracy: 0.8406\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"import pickle\nimport numpy as np\n\n# Load the saved model and label encoder\nwith open(\"Integrity_Impact_label_encoder.pkl\", 'rb') as f:\n    label_encoder = pickle.load(f)\n\nwith open(\"Integrity_Impact_best_xgboost_model.pkl\", 'rb') as f:\n    best_model = pickle.load(f)\n\n# Select a random sample from the test set\nrandom_index = np.random.randint(0, X_test.shape[0])\nrandom_sample = X_test[random_index:random_index + 1]\n\n# Predict the label for the sample (CatBoost requires data on CPU)\npredicted_label_encoded = best_model.predict(random_sample)\n\n# Convert the encoded prediction back to the original label\npredicted_label = label_encoder.inverse_transform(predicted_label_encoded.astype(int))\n\n# Get the actual label for the sample\nactual_label_encoded = y_test[random_index]\nactual_label = label_encoder.inverse_transform([actual_label_encoded])\n\n# Display the actual and predicted values\nprint(f\"Actual label: {actual_label[0]}\")\nprint(f\"Predicted label: {predicted_label[0]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:39:34.365864Z","iopub.execute_input":"2024-11-06T08:39:34.366290Z","iopub.status.idle":"2024-11-06T08:39:34.502744Z","shell.execute_reply.started":"2024-11-06T08:39:34.366253Z","shell.execute_reply":"2024-11-06T08:39:34.501996Z"}},"outputs":[{"name":"stdout","text":"Actual label: LOW\nPredicted label: LOW\n","output_type":"stream"}],"execution_count":55},{"cell_type":"markdown","source":"# Predicting Vulnerability Impact Metrics","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport pickle\nimport numpy as np\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Load your DataFrame\n# df = pd.read_csv('your_dataset.csv')  # Uncomment and specify your DataFrame source\n\n# Define columns to exclude\ncolumns_to_exclude = [\n    'Impact_Score', 'CVE_ID', 'ASSIGNER', 'Description', \n    'Published_Date', 'Last_Modified_Date', \n    'Access_Vector', 'Access_Complexity', \n    'Configurations', 'Reference_Data',\n    'Year', 'Base_Score', 'Exploitability_Score', \n    'Confidentiality_Impact', 'Integrity_Impact', \n    'Availability_Impact'\n]\n\n# List of target variables\ntarget_variables = [\n    'Impact_Score',\n    'Base_Score',\n    'Exploitability_Score',\n    'Access_Complexity',\n    'Access_Vector',\n    'Availability_Impact',\n    'Confidentiality_Impact',\n    'Integrity_Impact'\n]\n\n# Function to split dataset and return train-test splits\ndef train_test_split_for_target(target):\n    # Separate the target variable\n    y = df[target]\n    \n    # Select all columns except the excluded ones\n    X = df.drop(columns=columns_to_exclude)\n    \n    # If the target is categorical, encode it\n    if target in ['Access_Complexity', 'Access_Vector', 'Availability_Impact', 'Confidentiality_Impact', 'Integrity_Impact']:\n        label_encoder = LabelEncoder()\n        y = label_encoder.fit_transform(y)\n        return train_test_split(X, y, test_size=0.2, random_state=42), label_encoder  # Return encoder for later use\n    else:\n        return train_test_split(X, y, test_size=0.2, random_state=42), None\n\n# Load regression models\nregression_models = {\n    \"Impact_Score\": \"impact_Score_xgb_regressor.pkl\",\n    \"Base_Score\": \"base_score_xgb_regressor.pkl\",\n    \"Exploitability_Score\": \"exploitability_Score_xgb_regressor.pkl\"\n}\n\n# Load classification models and label encoders\nclassification_models = {\n    \"Access_Complexity\": (\"Access_Complexity_best_catboost_model.pkl\", \"Access_Complexity_label_encoder.pkl\"),\n    \"Access_Vector\": (\"accessVector_best_catboost_model.pkl\", \"Access_Vector_label_encoder.pkl\"),\n    \"Availability_Impact\": (\"Availability_Impact_best_catboost_model.pkl\", \"Availability_Impact_label_encoder.pkl\"),\n    \"Confidentiality_Impact\": (\"Confidentiality_Impact_best_xgboost_model.pkl\", \"Confidentiality_Impact_label_encoder.pkl\"),\n    \"Integrity_Impact\": (\"Integrity_Impact_best_xgboost_model.pkl\", \"Integrity_Impact_label_encoder.pkl\"),\n}\n\n# Initialize dictionaries to hold actual and predicted values\nactual_outputs = {}\npredicted_outputs = {}\n\n# Loop through all target variables to get train-test splits and make predictions\nfor target in target_variables:\n    (X_train, X_test, y_train, y_test), label_encoder = train_test_split_for_target(target)\n\n    # Reset index of y_test and X_test if necessary\n    y_test = pd.Series(y_test).reset_index(drop=True)\n    X_test = pd.DataFrame(X_test).reset_index(drop=True)\n\n    # Select a random row from the test set\n    random_index = np.random.randint(0, len(X_test))\n    random_input = X_test.iloc[random_index].values.reshape(1, -1)\n\n    # Predict for regression models\n    if target in regression_models.keys():\n        model_file = regression_models[target]\n        with open(model_file, 'rb') as file:\n            loaded_model = pickle.load(file)\n        real_output = y_test.iloc[random_index]\n        predicted_output = loaded_model.predict(random_input)\n        actual_outputs[target] = real_output\n        predicted_outputs[target] = predicted_output[0]\n\n    # Predict for classification models\n    elif target in classification_models.keys():\n        model_file, encoder_file = classification_models[target]\n        with open(encoder_file, 'rb') as f:\n            label_encoder = pickle.load(f)\n        with open(model_file, 'rb') as f:\n            best_model = pickle.load(f)\n\n        predicted_label_encoded = best_model.predict(random_input)\n        predicted_label = label_encoder.inverse_transform(predicted_label_encoded.astype(int))\n        actual_label_encoded = y_test.iloc[random_index]\n        actual_label = label_encoder.inverse_transform([actual_label_encoded])\n\n        actual_outputs[target] = actual_label[0]\n        predicted_outputs[target] = predicted_label[0]\n\n# Display actual and predicted values for all outputs\nprint(\"Actual Outputs:\")\nfor key, value in actual_outputs.items():\n    print(f\"{key}: {value}\")\n\nprint(\"\\nPredicted Outputs:\")\nfor key, value in predicted_outputs.items():\n    print(f\"{key}: {value}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T08:41:17.097014Z","iopub.execute_input":"2024-11-06T08:41:17.097992Z","iopub.status.idle":"2024-11-06T08:41:23.280298Z","shell.execute_reply.started":"2024-11-06T08:41:17.097949Z","shell.execute_reply":"2024-11-06T08:41:23.279485Z"}},"outputs":[{"name":"stdout","text":"Actual Outputs:\nImpact_Score: 3.6\nBase_Score: 6.949999999999999\nExploitability_Score: 2.95\nAccess_Complexity: MEDIUM\nAccess_Vector: NETWORK\nAvailability_Impact: HIGH\nConfidentiality_Impact: HIGH\nIntegrity_Impact: HIGH\n\nPredicted Outputs:\nImpact_Score: 3.6547837257385254\nBase_Score: 6.902083396911621\nExploitability_Score: 4.5355987548828125\nAccess_Complexity: MEDIUM\nAccess_Vector: NETWORK\nAvailability_Impact: HIGH\nConfidentiality_Impact: HIGH\nIntegrity_Impact: HIGH\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}